{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f6ec8e-fc7e-48fd-ae2c-84b501882d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # install al lthis stuff\n",
    "# !pip install opencv-python\n",
    "# !pip install ultralytics\n",
    "# !pip install pillow\n",
    "# !pip install faiss-cpu\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1aeac4-b821-430e-9f70-d7f45be7e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BeitImageProcessor, BeitModel\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de55201-02fb-4df8-8615-5d700e391a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEiT Embedding because this article used it:\n",
    "# https://medium.com/@nathanjjacob/how-to-build-a-logo-detection-recognition-system-at-scale-c2b094ae4fd2\n",
    "\n",
    "# This basically turns an image into a vector\n",
    "class BEiTEmbedding:\n",
    "    def __init__(self, model_name=\"microsoft/beit-base-patch16-224\"):\n",
    "        # Load the BEiT model from Hugging Face\n",
    "        self.feature_extractor = BeitImageProcessor.from_pretrained(model_name)  # Corrected class name\n",
    "        self.model = BeitModel.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Use average pooling\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class CLIPEmbedding:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.get_image_features(**inputs)\n",
    "        return outputs.squeeze().numpy()\n",
    "# ResNet Embedding\n",
    "class ResNetEmbedding:\n",
    "    def __init__(self, model_name=\"resnet50\"):\n",
    "        # Load the ResNet model from torchvision\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        img = self.transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model(img)  # Forward pass to get the embedding\n",
    "        return embedding.squeeze().cpu().numpy()  # Remove batch dimension and return as numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195dfa54-ff27-494c-8044-8e45229a577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # this function is from sklearn. Thank you sklearn :)\n",
    "    # this does the math for us\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    # return euclidean_distances(embedding1, embedding2)\n",
    "  \n",
    "def compute_euclidean_distances(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # this function is from sklearn. Thank you sklearn :)\n",
    "    # this does the math for us\n",
    "    # return cosine_similarity(embedding1, embedding2)\n",
    "    return euclidean_distances(embedding1, embedding2)[0][0]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714459ad-bc6a-46c9-a547-b046551709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logo_regions(image_path, model, bounding_box_threshold=0.0):\n",
    "    # Returns all of the logos found within an image\n",
    "\n",
    "    img = cv2.imread(image_path)  # Read the image here\n",
    "    results = model(img)\n",
    "    logo_regions = []\n",
    "    bounding_boxes = []\n",
    "    counter = 1\n",
    "    for box in results[0].boxes:\n",
    "        xyxy = box.xyxy[0].tolist()\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        confidence = box.conf[0].item()  # Get the confidence score for the detection\n",
    "\n",
    "        # print(f'bounding box confidence: {confidence}')\n",
    "        if confidence >= bounding_box_threshold:\n",
    "          cropped_logo = img[y1:y2, x1:x2]  # Extract detected region\n",
    "        \n",
    "\n",
    "          if cropped_logo.size > 0:\n",
    "            \n",
    "            height, width = cropped_logo.shape[:2]\n",
    "\n",
    "            # Calculate new width while maintaining aspect ratio\n",
    "            new_height = 224\n",
    "            new_width = int((new_height / height) * width)\n",
    "            \n",
    "            # Resize while keeping aspect ratio\n",
    "            resized_logo = cv2.resize(cropped_logo, (new_width, new_height))\n",
    "\n",
    "            cv2.imwrite(f'./cropped/cropped_{counter}_{image_path}', resized_logo)\n",
    "            counter += 1\n",
    "            # Convert grayscale to 3-channel (RGB-like) image\n",
    "            # three_channel_logo = cv2.cvtColor(resized_logo, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            logo_regions.append(resized_logo)\n",
    "            bounding_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "    return logo_regions, bounding_boxes, img  # Return img here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4701cc-a6cd-4100-b925-0285f3c11ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logo_embeddings(input_path, reference_path, model, embedding_models, thresholds, score_threshold):\n",
    "\n",
    "  '''\n",
    "  input_path is the file location of the main image\n",
    "  reference path is the file location of the reference image\n",
    "  model is what we use for general logo detection (this will always be YOLO in our case)\n",
    "  embedding_models will be an array of classes used to make our images vectors\n",
    "  thresholds is a dict of the cosine and euclidean thresholds the image embeddings should meet for each embedding model\n",
    "  score_threshold is a number of how many points an image needs to recieve for it to 'pass'\n",
    "  '''\n",
    "  # Extract logos and bounding boxes\n",
    "  input_logos, input_bboxes, input_img = extract_logo_regions(input_path, model)\n",
    "  reference_logos, _, _ = extract_logo_regions(reference_path, model)\n",
    "\n",
    "  if not input_logos or not reference_logos:\n",
    "    print(\"No logos detected in one or both images.\")\n",
    "    return\n",
    "\n",
    "  # Initialize score tracker\n",
    "  # Matrix of len(reference_logos) x len(input_logos). \n",
    "  # This keeps the scores separate for each reference logo found\n",
    "  scores = [[0] * len(input_logos) for _ in range(len(reference_logos))]\n",
    "\n",
    "  # For each embedding model\n",
    "  for feature_extractor in embedding_models:\n",
    "    # Get the name of the embedding model so we can index the thresholds dict\n",
    "    model_name = type(feature_extractor).__name__\n",
    "\n",
    "    # Get model-specific thresholds\n",
    "    cosine_threshold = thresholds[model_name][\"cosine\"]\n",
    "    euclidean_threshold = thresholds[model_name][\"euclidean\"]\n",
    "\n",
    "    # Maybe send to user 'Embedding images' with a loading screen here\n",
    "    # Compute embeddings and put them into an array\n",
    "    input_embeddings = [feature_extractor.extract_embedding(Image.fromarray(logo)) for logo in input_logos]\n",
    "    reference_embeddings = [feature_extractor.extract_embedding(Image.fromarray(logo)) for logo in reference_logos]\n",
    "\n",
    "    # Maybe send to user 'iterating through images'\n",
    "    # Iterate through each embeddings (basically each logo)\n",
    "    for i, ref_embedding in enumerate(reference_embeddings):\n",
    "      for j, input_embedding in enumerate(input_embeddings):\n",
    "        # Compute similarity scores\n",
    "        cosine_sim = compute_cosine_similarity(input_embedding, ref_embedding)\n",
    "        euclidean_dist = compute_euclidean_distances(input_embedding, ref_embedding)\n",
    "\n",
    "        # Check if similarities meet the model specific thresholds\n",
    "        # Again, scores is a 2d array. Rows = num of reference images, cols = num of main image\n",
    "        if cosine_sim >= cosine_threshold:\n",
    "          scores[i][j] += 1 # Plus 1 if cosine sim is met\n",
    "        if euclidean_dist <= euclidean_threshold:\n",
    "          scores[i][j] += 1 # Plus 1 if euclidean distance is met\n",
    "\n",
    "        # Print what the scores are\n",
    "        print(f'{model_name} score: {scores[i][j]}')\n",
    "\n",
    "\n",
    "\n",
    "  # Final decision: Classify as match if score is at least 4/6\n",
    "  for i in range(len(reference_logos)):  # Iterate over reference logos (rows)\n",
    "    for j in range(len(input_logos)):  # Iterate over input logos (columns)\n",
    "      \n",
    "      if scores[i][j] >= score_threshold:  # Check per reference logo\n",
    "        x1, y1, x2, y2 = input_bboxes[j]\n",
    "        color = (255, 255, 255)  # White bounding box. We can change this later if needed\n",
    "        cv2.rectangle(input_img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "  # Save output image with bounding boxes\n",
    "  cv2.imwrite(\"output_image.jpg\", input_img)\n",
    "  print(\"Processed image saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d52c4c-b5c5-4792-a151-df3a5538c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEiT\n",
      "\n",
      "0: 448x640 3 logos, 77.7ms\n",
      "Speed: 4.0ms preprocess, 77.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 logo, 94.4ms\n",
      "Speed: 5.0ms preprocess, 94.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "BEiTEmbedding score: 0\n",
      "BEiTEmbedding score: 0\n",
      "BEiTEmbedding score: 1\n",
      "CLIPEmbedding score: 0\n",
      "CLIPEmbedding score: 0\n",
      "CLIPEmbedding score: 3\n",
      "ResNetEmbedding score: 0\n",
      "ResNetEmbedding score: 0\n",
      "ResNetEmbedding score: 3\n",
      "Processed image saved.\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"starbucks.jpg\"  # Change this to your image file\n",
    "reference_image_path = \"starbucks4.png\"\n",
    "yolo_model = YOLO(\"../488_back/best.pt\")  # Load your YOLO model\n",
    "\n",
    "embedding_models = [BEiTEmbedding(), CLIPEmbedding(), ResNetEmbedding()]\n",
    "thresholds = {\n",
    "  'BEiTEmbedding': {'cosine': .3, 'euclidean': 110},\n",
    "  'CLIPEmbedding': {'cosine': .65, 'euclidean': 7.5},\n",
    "  'ResNetEmbedding': {'cosine': .75, 'euclidean': 50}\n",
    "}\n",
    "\n",
    "\n",
    "print('BEiT')\n",
    "compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, embedding_models, thresholds, 4)\n",
    "\n",
    "# print(f'\\nCLIP \\n---------------------------------------------------------------------------')\n",
    "# compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, CLIPEmbedding())\n",
    "\n",
    "# print(f'\\nResNet \\n-------------------------------------------------------------------------')\n",
    "# compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, ResNetEmbedding())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aaa3d-fbfe-4870-96f0-254bc3f749a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b73e82-d652-4e68-ac92-37abe1297f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
