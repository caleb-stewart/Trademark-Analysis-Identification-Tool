{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f6ec8e-fc7e-48fd-ae2c-84b501882d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # install al lthis stuff\n",
    "# !pip install opencv-python\n",
    "# !pip install ultralytics\n",
    "# !pip install pillow\n",
    "# !pip install faiss-cpu\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1aeac4-b821-430e-9f70-d7f45be7e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BeitImageProcessor, BeitModel\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de55201-02fb-4df8-8615-5d700e391a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEiT Embedding because this article used it:\n",
    "# https://medium.com/@nathanjjacob/how-to-build-a-logo-detection-recognition-system-at-scale-c2b094ae4fd2\n",
    "\n",
    "# This basically turns an image into a vector\n",
    "class BEiTEmbedding:\n",
    "    def __init__(self, model_name=\"microsoft/beit-base-patch16-224\"):\n",
    "        # Load the BEiT model from Hugging Face\n",
    "        self.feature_extractor = BeitImageProcessor.from_pretrained(model_name)  # Corrected class name\n",
    "        self.model = BeitModel.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Use average pooling\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class CLIPEmbedding:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.get_image_features(**inputs)\n",
    "        return outputs.squeeze().numpy()\n",
    "# ResNet Embedding\n",
    "class ResNetEmbedding:\n",
    "    def __init__(self, model_name=\"resnet50\"):\n",
    "        # Load the ResNet model from torchvision\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def extract_embedding(self, img):\n",
    "        img = self.transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model(img)  # Forward pass to get the embedding\n",
    "        return embedding.squeeze().cpu().numpy()  # Remove batch dimension and return as numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195dfa54-ff27-494c-8044-8e45229a577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # this function is from sklearn. Thank you sklearn :)\n",
    "    # this does the math for us\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    # return euclidean_distances(embedding1, embedding2)\n",
    "  \n",
    "def compute_euclidean_distances(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # this function is from sklearn. Thank you sklearn :)\n",
    "    # this does the math for us\n",
    "    # return cosine_similarity(embedding1, embedding2)\n",
    "    return euclidean_distances(embedding1, embedding2)[0][0]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714459ad-bc6a-46c9-a547-b046551709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logo_regions(image_path, model, bounding_box_threshold=0.0):\n",
    "    # Returns all of the logos found within an image\n",
    "\n",
    "    img = cv2.imread(image_path)  # Read the image here\n",
    "    results = model(img)\n",
    "    logo_regions = []\n",
    "    bounding_boxes = []\n",
    "    counter = 1\n",
    "    for box in results[0].boxes:\n",
    "        xyxy = box.xyxy[0].tolist()\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        confidence = box.conf[0].item()  # Get the confidence score for the detection\n",
    "\n",
    "        # print(f'bounding box confidence: {confidence}')\n",
    "        if confidence >= bounding_box_threshold:\n",
    "          cropped_logo = img[y1:y2, x1:x2]  # Extract detected region\n",
    "        \n",
    "\n",
    "          if cropped_logo.size > 0:\n",
    "            \n",
    "            height, width = cropped_logo.shape[:2]\n",
    "\n",
    "            # Calculate new width while maintaining aspect ratio\n",
    "            new_height = 224\n",
    "            new_width = int((new_height / height) * width)\n",
    "            \n",
    "            # Resize while keeping aspect ratio\n",
    "            resized_logo = cv2.resize(cropped_logo, (new_width, new_height))\n",
    "\n",
    "            cv2.imwrite(f'./cropped/cropped_{counter}_{image_path}', resized_logo)\n",
    "            counter += 1\n",
    "            # Convert grayscale to 3-channel (RGB-like) image\n",
    "            # three_channel_logo = cv2.cvtColor(resized_logo, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            logo_regions.append(resized_logo)\n",
    "            bounding_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "    return logo_regions, bounding_boxes, img  # Return img here\n",
    "\n",
    "def compare_logo_embeddings(input_path, reference_path, model, feature_extractor, similarity_threshold=0.0):\n",
    "    #compare logos using embeddings\n",
    "    \n",
    "    # Extract logos and bounding boxes, also get the image\n",
    "    input_logos, input_bboxes, input_img = extract_logo_regions(input_path, model)\n",
    "    reference_logos, reference_bboxes, reference_img = extract_logo_regions(reference_path, model)\n",
    "    \n",
    "    if not input_logos or not reference_logos:\n",
    "        print(\"No logos detected in one or both images.\")\n",
    "        return\n",
    "    \n",
    "    # get embeddings (vectors) from the reference and input image\n",
    "    input_embeddings = [feature_extractor.extract_embedding(Image.fromarray(input_logo)) for input_logo in input_logos]\n",
    "    reference_embeddings = [feature_extractor.extract_embedding(Image.fromarray(reference_logo)) for reference_logo in reference_logos]\n",
    "    \n",
    "    # Compare logos using cosine similarity\n",
    "    for index, input_embedding in enumerate(input_embeddings):\n",
    "        for ref_index, reference_embedding in enumerate(reference_embeddings):\n",
    "            cos_similarity = compute_cosine_similarity(input_embedding, reference_embedding)\n",
    "            euc_dist = compute_euclidean_distances(input_embedding, reference_embedding)\n",
    "\n",
    "            print(f'cosine similarity score: {cos_similarity}')\n",
    "            print(f'euclidean distance score: {euc_dist}\\n')\n",
    "            # If similarity is above the threshold, then they match\n",
    "            if cos_similarity >= similarity_threshold:\n",
    "                # Get the bounding box for the matching input logo\n",
    "                x1, y1, x2, y2 = input_bboxes[index]\n",
    "                color = [255, 255, 255]\n",
    "                cv2.rectangle(input_img, (x1, y1), (x2, y2), color, 2)  # Draw on the input image\n",
    "\n",
    "    # Save the processed image after drawing the rectangles\n",
    "    cv2.imwrite(\"output_image.jpg\", input_img)\n",
    "    print(\"Processed image saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56d52c4c-b5c5-4792-a151-df3a5538c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEiT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 logo, 107.5ms\n",
      "Speed: 6.1ms preprocess, 107.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 logo, 125.4ms\n",
      "Speed: 6.3ms preprocess, 125.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "cosine similarity score: 0.23492184281349182\n",
      "euclidean distance score: 130.15493774414062\n",
      "\n",
      "Processed image saved.\n",
      "\n",
      "CLIP \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "0: 448x640 1 logo, 118.6ms\n",
      "Speed: 5.3ms preprocess, 118.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 logo, 126.8ms\n",
      "Speed: 6.5ms preprocess, 126.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "cosine similarity score: 0.8996052742004395\n",
      "euclidean distance score: 5.047374725341797\n",
      "\n",
      "Processed image saved.\n",
      "\n",
      "ResNet \n",
      "-------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\caleb\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 logo, 121.9ms\n",
      "Speed: 4.3ms preprocess, 121.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 logo, 125.2ms\n",
      "Speed: 5.4ms preprocess, 125.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "cosine similarity score: 0.7854627370834351\n",
      "euclidean distance score: 48.24747085571289\n",
      "\n",
      "Processed image saved.\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"starbucks2.jpg\"  # Change this to your image file\n",
    "reference_image_path = \"starbucks4.png\"\n",
    "yolo_model = YOLO(\"../488_back/best.pt\")  # Load your YOLO model\n",
    "\n",
    "print('BEiT')\n",
    "compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, BEiTEmbedding())\n",
    "\n",
    "print(f'\\nCLIP \\n---------------------------------------------------------------------------')\n",
    "compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, CLIPEmbedding())\n",
    "\n",
    "print(f'\\nResNet \\n-------------------------------------------------------------------------')\n",
    "compare_logo_embeddings(input_image_path, reference_image_path, yolo_model, ResNetEmbedding())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aaa3d-fbfe-4870-96f0-254bc3f749a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b73e82-d652-4e68-ac92-37abe1297f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
